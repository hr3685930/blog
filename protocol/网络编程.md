# 网络编程
第一就是理解网络协议，并在这个基础上和操作系统内核配合，感知各种网络 I/O 事件；第二就学会使用线程处理并发。

## Socket(TCP)
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/81CDDB4F-392E-4710-90A9-DD24E0A1D324.png)
服务器端通过创建 socket，bind，listen 完成初始化，通过 accept 完成连接的建立。
客户端通过场景 socket，connect 发起连接建立请求。


Sockfd里的这个fd代表什么?
file description，文件描述符。UNIX世界里万物皆文件。

服务端

1. 创建socket
int socket(int domain, int type, int protocol)

2. bind
bind(int fd, sockaddr * addr, socklen_t len)

3. listen
int listen (int socketfd, int backlog)

4. accept
int accept(int listensockfd, struct sockaddr *cliaddr, socklen_t *addrlen)

客户端

1. 创建socket
int socket(int domain, int type, int protocol)

2. connect
int connect(int sockfd, const struct sockaddr *servaddr, socklen_t addrlen)

![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/D02D4277-4EF6-475F-9838-A64D14D9F4C2.png)
### TCP三次握手
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/33678EE4-8131-4917-8079-56AF9010DAE2.png)

###  发送数据
ssize_t write (int socketfd, const void *buffer, size_t size)

ssize_t send (int socketfd, const void *buffer, size_t size, int flags)

ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags)

### 读取数据
ssize_t read (int socketfd, void *buffer, size_t size)

对于 send 来说，返回成功仅仅表示数据写到发送缓冲区成功，并不表示对端已经成功收到。
对于 read 来说，需要循环读取数据，并且需要考虑 EOF 等异常条件。


## Socket(UDP)
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/39F26920-F99B-4273-826B-4AC51C40264F.png)
UDP 程序通过 recvfrom 和 sendto 函数直接收发数据报报文。



ssize_t recvfrom(int sockfd, void *buff, size_t nbytes, int flags, 
                    struct sockaddr *from, socklen_t *addrlen); 
                    
ssize_t sendto(int sockfd, const void *buff, size_t nbytes, int flags,
                const struct sockaddr *to, socklen_t *addrlen);

### TCP和UDP区别
TCP 客户端的 connect 函数会直接返回“Connection refused”报错信息。而在 UDP 程序里，如果我们只运行客户端，程序会一直阻塞在 recvfrom 上。

UDP服务器端重启后可以继续收到客户端的报文，这在 TCP 里是不
可以的，TCP 断联之后必须重新连接才可以发送报文信息


## Socket(IPC)本地套接字
本地字节流套接字和 TCP 服务器端、客户端编程最大的差异就是
套接字类型的不同。本地字节流套接字识别服务器不再通过 IP 地址和端口，而是通过本地文件

本地套接字的编程接口和 IPv4、IPv6 套接字编程接口是一致的，可以支持字节流和数据报两种协议。
本地套接字的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报套接字实现。


一段数据流从应用程序发送端到应用程序接收端,总共经过多少次拷贝？
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/C3D2F009-9F17-4C4D-954B-62DF94C27F8A.png)


## TIME_WAIT(4次挥手)
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/E5689234-C4A5-4A2D-B5FD-B63B070001C0.png)
TIME_WAIT 的引入是为了让 TCP 报文得以自然消失，同时为了让被动关闭方能够正常关闭；
通信双方建立TCP连接后，主动关闭连接的一方就会进入TIME_WAIT状态。
Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。

排查TIME_WAIT
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
查看TIME_WAIT连接数
netstat -ae|grep "TIME_WAIT"

通过调整内核参数解决
vi /etc/sysctl.conf
```
net.ipv4.tcp_syncookies = 1  ##开启SYN Cookies
net.ipv4.tcp_tw_reuse = 1  ##开启重用
net.ipv4.tcp_tw_recycle = 1  ##开启TCP连接中TIME-WAIT sockets的快速回收
net.ipv4.tcp_fin_timeout = 30 ##修改系統默认的TIMEOUT时间
```
然后执行/sbin/sysctl -p让参数生效。

### close和shutdown
Close 函数只是把套接字引用计数减 1，未必会立即关闭连接；
Close 函数如果在套接字引用计数达到 0 时，立即终止读和写两个方向的数据传送。
在期望关闭连接其中一个方向时，应该使用 shutdown 函数。
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/CACFBDA4-38AE-4C10-87F3-8719848D076A.png)
- Shutdown是一种优雅地单方向或者双方向关闭socket的方法。 而close则立即双方向强制关闭socket并释放相关资源。
* 如果有多个进程共享一个socket，shutdown影响所有进程，而close只影响本进程。
在大多数情况下，我们会优选 shutdown 来完成对连接一个方向的关闭，待对端处理完之后，再完成另外一个方向的关闭。


## TCP探活
- 应用层面的心跳机制
- TCP协议自带的保活功能keep-alive
两种方式的区别
TCP协议自带的keep-alive保活功能
使用起来简单, 减少了应用层代码的复杂度. 推测也会更节省流量, 因为一般来说应用层的数据传输到协议层时都会被加上额外的包头包尾. 由TCP协议提供的检活, 其发的探测包, 理论上实现的会更精妙(用更少的字节完成更多的目标), 耗费更少的流量.

应用层心跳的好处
一是比较灵活, 因为协议层的心跳只能提供最纯粹的检活功能, 但是应用层自己可以随意控制
二是通用, 应用层的心跳不依赖协议. 如果有一天不用TCP要改为UDP了

## 小数据包发送策略
发送窗口用来控制发送和接收端的流量；阻塞窗口用来控制多条连接公平使用的有限带宽。
小数据包加剧了网络带宽的浪费，为了解决这个问题，引入了如 Nagle 算法、延时 ACK等机制。
在程序设计层面，不要多次频繁地发送小报文，如果有，可以使用 writev 批量发送。  最多支持1024个元素

## UDP的connect
对 UDP使用 connect，绑定本地地址和端口，是为了让我们的程序可以快速获取异步错误信息的通知，同时也可以获得一定性能上的提升。

不使用 connect 方式，每次发送报文都会需要这样的过程：
> 连接套接字→发送报文→断开套接字→连接套接字→………  

使用 connect 方式，就会变成下面这样：
> 连接套接字→发送报文→发送报文→……→最后断开套接字  

## SO_REUSEADDR(服务端端口复用)
setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
服务器端程序，都应该设置 SO_REUSEADDR 套接字选项，以便服务端程序可以在极短时间内复用同一个端口启动。

tcp_tw_reuse和SO_REUSEADDR的区别
tcp_tw_reuse 是内核选项，主要用在连接的发起方。TIME_WAIT 状态的连接创建时间超过 1 秒后，新的连接才可以被复用，注意，这里是连接的发起方；

SO_REUSEADDR 是用户态的选项，SO_REUSEADDR 选项用来告诉操作系统内核，如果端口已被占用，但是 TCP 连接状态位于 TIME_WAIT ，可以重用端口。如果端口忙，而 TCP 处于其他状态，重用端口时依旧得到“Address already in use”的错误信息。注意，这里一般都是连接的服务方。


## TCP的字节流
Tcp是字节流，没有明显的界限，所以当缓存区满了之后，网卡就会把内容传输到服务端，而服务端也并没有明确知道这些流应该怎么分割，所以当多个字节流由于某种原因粘在了一起，那么就会出现了内容错误的情况了。

粘包解决方案
- 消息体长度标识
- 在末尾加上统一的标识符，比如换行符或者其他约定的
- 每个消息都使用固定的长度

## TCP故障模式总结
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/A3ED78F0-9CD1-4841-923E-69BDC01ED9A1.png)
### 网络中断
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/1AF06BB6-94B9-460A-94AE-A44C1619D357.png)
### 系统崩溃
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/C7DE7F29-DBDD-4A18-964D-A5CB3360D637.png)
### 没有read
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/0D37F7B6-C932-430D-8BCA-FFFBE8DF30F0.png)

故障分为两大类
一类是对端无 FIN 包，需要通过巡检或超时来发现；
另一类是对端有 FIN 包发出，需要通过增强 read或 write 操作的异常处理，帮助我们发现此类异常。

我们需要记得为 SIGPIPE 注册处理函数，通过 write 操作感知 RST 的错误信息，这样可以保证我们的应用程序在 Linux 4.4 和 Mac OS 上都能正常处理异常。


## 异常状况
- 缓冲区溢出
- 指针错误
- 连接超时检测

## 多路复用 I/O 
select  [时间复杂度O(n)]
内核需要将消息传递到用户空间，都需要内核拷贝动作
在 Linux 系统中，select 的默认最大值为 1024。

poll    [时间复杂度O(n)]
Poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是它没有最大连接数的限制，原因是它是基于链表来存储的.

epoll    [时间复杂度O(1)]
Epoll 通过改进的接口设计，避免了用户态 - 内核态频繁的数据拷贝，大大提高了系统性能。在使用 epoll 的时候，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们.

### epoll原理
Epoll 维护了一棵红黑树来跟踪所有待检测的文件描述字，黑红树的使用减少了内核和用户空间大量的数据拷贝和内存分配，大大提高了性能。同时，epoll 维护了一个链表来记录就绪事件，内核在每个文件有事件发生时将自己登记到这个就绪事件列表中，通过内核自身的文件 file-eventpoll 之间的回调和唤醒机制，减少了对内核描述字的遍历，大大加速了事件通知和检测的效率，这也为 level-triggered 和edge-triggered 的实现带来了便利。
通过对比 poll/select 的实现，我们发现 epoll 确实克服了 poll/select 的种种弊端，不愧是 Linux 下高性能网络编程的皇冠。我们应该感谢 Linux 社区的大神们设计了这么强大的事件分发机制，让我们在 Linux 下可以享受高性能网络服务器带来的种种技术红利。


## 非阻塞 I/O 
非阻塞 I/O 可以被用到读操作、写操作、接收连接操作和发起连接操作上
![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/218BFFA9-2FCF-4914-94A7-DB9DCDF10F7E.png)
非阻塞 I/O 可以使用在 read、write、accept、connect 等多种不同的场景，在非阻塞I/O 下，使用轮询的方式引起 CPU 占用率高，所以一般将非阻塞 I/O 和 I/O 多路复用技术Select、poll 等搭配使用，在非阻塞 I/O 事件发生时，再调用对应事件的处理函数。这种方式，极大地提高了程序的健壮性和稳定性，是 Linux 下高性能网络编程的首选。


多路复用IO为何比非阻塞IO模型的效率高?
是因为在非阻塞IO中，不断地询问socket状态是通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多


## C10K
一台主机上同时支持 1 万个连接
- 文件句柄
- 系统内存
假设每个连接需要 128K 的缓冲区，那么 1 万个链接就需要大约 1.2G 的应用层缓冲

- 网络带宽
假设 1 万个连接，每个连接每秒传输大约 1KB 的数据，那么带宽需要 10000x 1KB/s x 8 = 80Mbps

在 Linux 下，解决高性能问题的利器是非阻塞 I/O 加上 epoll 机制，再利用多线程能力。

## 阻塞 I/O 的进程模型
父进程和子进程
> 使用函数 fork 就可以  

使用阻塞 I/O 和进程模型，为每一个连接创建一个独立的子进程来进行服务，是一个非常简单有效的实现方式，这种方式可能很难足高性能程序的需求，但好处在于实现简单。在实现这样的程序时，我们需要注意两点:
	- 要注意对套接字的关闭梳理；
	- 要注意对子进程进行回收，避免产生不必要的僵尸进程。


## 阻塞I/O和线程模型
既然可以使用多进程来处理并发，为什么还要使用多线程模型呢？
在同一个进程下，线程上下文切换的开销要比进程小得多。

## 多路复用I/O和线程模型
poll和多线程模型
主从 reactor 模式
主从 reactor 模式中，主 reactor 只负责连接建立的处理，而把已连接套接字的 I/O 事件分发交给从 reactor 线程处理，这大大提高了客户端连接的处理能力

epoll和多线程模型
和 poll 相比，epoll 从事件集合和就绪列表两个方面加强了程序性能，是 Linux 下高性能网络程序的首选。

无论是 Reactor 模式，还是 Proactor 模式，都是一种基于事件分发的网络编程模式。
Reactor 模式是基于待完成的 I/O 事件，而 Proactor 模式则是基于已完成的 I/O 事件

## 异步IO
和同步 I/O 相比，异步 I/O 的读写动作由内核自动完成，不过，在 Linux 下目前仅仅支持简单的基于本地文件的 aio 异步操作，这也使得我们在编写高性能网络程序时，首选Reactor 模式，借助 epoll 这样的 I/O 分发技术完成开发；而 Windows 下的 IOCP 则是一种异步 I/O 的技术，并由此产生了和 Reactor 齐名的 Proactor 模式，借助这种模式，可以完成 Windows 下高性能网络程序设计。


## 阻塞 / 非阻塞 VS 同步 / 异步
- 第一种是阻塞 I/O。
阻塞 I/O 发起的 read 请求，线程会被挂起，一直等到内核数据准备好，并把数据从内核区域拷贝到应用程序的缓冲区中，当拷贝过程完成，read 请求调用才返回。接下来，应用程序就可以对缓冲区的数据进行数据解析。
- 第二种是非阻塞 I/O。
非阻塞的 read 请求在数据未准备好的情况下立即返回，应用程序可以不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲，并完成这次 read 调用。注意，这里最后一次 read 调用，获取数据的过程，是一个同步的过程。这里的同步指的是内核区域的数据拷贝到缓存区这个过程。每次让应用程序去轮询内核的 I/O 是否准备好，是一个不经济的做法，因为在轮询的过程
中应用进程啥也不能干。于是，像 select、poll 这样的 I/O 多路复用技术就隆重登场了。通过 I/O 事件分发，当内核数据准备好时，再通知应用程序进行操作。这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。注意，这里 read 调用，获取数据的过程，也是一个同步的过程。
- 异步IO
当我们发起 aio_read 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。


![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/80AB8849-829F-407E-A2B9-70B892EFC384.png)



## 几种IO模型对比

### Redis 
Redis是基于内存的采用单进程和单线程模型的KV数据库
Redis 单线程为什么还能这么快？
* 纯内存访问，所有数据都在内存中，所有的运算都是内存级别的运算，内存响应时间的时间为纳秒级别，这是redis达到万级每秒的基础。
- 采用单线程，避免了不必要的上下文切换和竞争条件；不存在多线程导致的切换而消耗CPU，不用考虑各种锁的问题，不存在加锁和释放锁的的操作，没有因为可能出现的死锁而导致的性能消耗。

### Nginx 和 php-fpm
Nginx和php-fpm都是多进程，一个进程只有一个线程；
Nginx一个线程是非阻塞/io多路复用/epoll模型，将请求分发后无需等待，仅监听回调结果
Php-fpm一个线程是阻塞模型，必须等待该客户端请求php服务端返回数据，下一个nginx发过来的请求才能被受理
PHP本身是单进程单线程的，它只是脚本语言。


## 扩展 (Actor和CSP)
ractor 和  proactor 上面已经写了,  但是并发模型可不止这两种 还有 Erlang的actor模型和 golang的CSP模型

![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/F7ED69C4-B529-4268-A4D1-4073AF5C60DB.png)
在Actor模型中，主角是Actor，类似一种worker，Actor彼此之间直接发送消息，不需要经过什么中介，消息是异步发送和处理的


![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/686FAC9D-D78D-44E8-95C3-BC57ECE16424.png)Go语言的CSP模型是由协程Goroutine与通道Channel实现


![](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/A93C8C56-6C99-49F5-9813-9CBCCDB56BED.png)